\documentclass[11pt]{article}

\usepackage{booktabs}
\usepackage{homeworkpkg}
\usepackage{enumitem}
\usepackage{xcolor,listings}
\usepackage{caption}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{multicol}

\graphicspath{ {images/} }

%% Local Macros and packages: add any of your own definitions here.

\begin{document}

% Homework number, your name and NetID, and optionally, the names/NetIDs of anyone you collaborated with. If you did not collaborate with anybody, leave the last parameter empty.
\homework
    {5}
    {Nestor Alejandro Bermudez Sarmiento (nab6)}
    {}


\section*{Introduction}

In this assignment we explore two topics: classifications methods and clustering methods.
For classification we will illustrate the use of a Naive Bayes classifier and also K-nearest neighbor using different values of $k$. Finally we will look at the k-means, DBSCAN and AGNES clustering algorithms.

\section*{Question 1}

Consider the following equations:\\
\begin{gather}
Pr(X, C) = Pr(X \mid C) \times Pr(C) \\	
Pr(C \mid X) = \frac{Pr(X \mid C) \times Pr(C)}{Pr(X)}
\end{gather}

\textbf{1a(i)}: $Pr$(Popularity = `P') = $\frac{7}{10}$. Done by simply counting the number of examples that match the criteria. Known as the prior probability.\\

\textbf{1a(ii)}: $Pr$(Popularity = `NP') = $\frac{3}{10}$. Found by taking the value from the previous question from 1 since this is a binary classification problem.\\

\textbf{1a(iii)}: By using equation 1 and class-conditional independence:
\[ Pr(\textrm{Price = `\$', Delivery = `Yes' , Cuisine = `Korean' \textbar{} Popularity = `P'}) \]
\[ = \prod_{i=1}^3 Pr(x_i \mid \textrm{Popularity = `P'}) \]
where $x_i$ is each of the features in the tuple considered. So:\\
\[ x_1 = \frac{4}{7}, x_2 = \frac{4}{7}, x_3 = \frac{2}{7} \] and then
\[ Pr(\textrm{Price = `\$', Delivery = `Yes' , Cuisine = `Korean' \textbar{} Popularity = `P'}) = \frac{4}{7} \times \frac{4}{7} \times \frac{2}{7} = \frac{32}{343} \]

\textbf{1a(iv)}: $Pr$(Price = `\$', Delivery = `Yes' , Cuisine = `Korean' \textbar{} Popularity = `NP')
\[ = \prod_{i=1}^3 Pr(x_i \mid \textrm{Popularity = `P'}) \]
where $x_i$ is each of the features in the tuple considered. So:\\
\[ x_1 = \frac{1}{3}, x_2 = \frac{2}{3}, x_3 = \frac{1}{3} \] and then
\[ Pr(\textrm{Price = `\$', Delivery = `Yes' , Cuisine = `Korean' \textbar{} Popularity = `P'}) = \frac{4}{7} \times \frac{4}{7} \times \frac{2}{7} = \frac{2}{27} \]

\textbf{1b}: When doing Naive Bayes classification one of the common ways to perform the prediction is to using \textbf{MAP}\footnote{\url{https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation}} (Maximum a posteriori). MAP indicates that we should calculate the probability of each class given the evidence. For this particular case, we have already done that in part 1a(iii) and 1a(iv). \\

So to answer the question it is just enough to compare the previous results. Since $\frac{32}{343} > \frac{2}{27}$ the answer to the question is \textbf{yes}, it will be class as Popular.\\

\textbf{1c}: the techniques we discussed in class can, theoretically, apply to Naive Bayes. That is: boosting, bagging and ensemble. There is no "one size fit all" solution and finding the right method usually involves trying multiple of them until you get a reasonable accuracy. One of the algorithms we can use is AdaBoost on top of Naive Bayes. This works as follows: \\

Let $d$ be the number of classes and $k$ the number of iterations or predictors in our ensemble method. These will be the hyper-parameters of our classifier.
\begin{enumerate}
\item make all tuples equally likely, e.g. assign the same weight to all of them. The usual value for the weigh is the inverse of the number of classes ($\frac{1}{d}$).
\item create a new data set $D_i$ by sampling with replacement from the original data set. Whether an example is included or not in $D_i$ is decided by its weigh. 
\item train a Naive Bayes classifier using $D_i$.
\item evaluate the Naive Bayes classifier and find its error rate. We can do it by using the test data or by using some of the examples that were not included in $D_i$. 
\item if the error is greater than a threshold (at least 50\%), reset the weighs and try again with a new sample (go back to step 2).
\item if the error is good enough then for every tuple in $D_i$ update its corresponding weigh by a factor of the $\frac{e}{acc}$ where $e$ is the error and $acc$ is the accuracy ($1 - e$).
\item repeat from step 2 if done less than $k$ times.
\end{enumerate}

Once the training is done we need to predict labels for new evidence \textbf{X}. To do so we initialize the weights with zero and on each iteration we update them by taking the log of the accuracy over the error rate. On each iteration we see what the prediction for the current classifier is and update the weight of that class only! At the end we predict the class with the highest weight.\\

\textbf{1d}: There are a few metrics that can help measure the performance of a classifier when one of the classes is rare.\\

i) Sensitivity: it measures the rate of true positives recognition and is defined by the formula: $\frac{TP}{P}$. Where TP is the total number of true positives (correctly classified as positive) and P is the total number of instances that were positive. Note that the formula does not include the negatives so it ignores the fact that the positives are rare because there is no point of comparison between positives and negatives.\\

ii) Specificity: measures how successful is the classifier in predicting false when over the examples that were indeed false. As an example, in a medical diagnosis, tests usually have high specificity, meaning that they rarely predict a positive in healthy patients, this doesn't say much about whether it was positive or not but it is a first step to perform more tests. Example inspired by this Wikipedia \href{https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity}{page}

\section*{Question 2}

\textbf{2a}: since $k = 1$ this is rather simple. For every example in the test data we calculate the distance to every exampled seen in the training data. The following table summarizes this and marks the minimum distance in green.
\begin{center}
\includegraphics[scale=0.75]{distances.png}
\end{center}

The first 4 rows in the matrix correspond to the training examples that belong to class +1 and the other half the ones that belong to class -1. So, the predicted classes for the test data are: -1, +1, -1 and -1 respectively. And the actual classes are +1, +1, -1, -1 so only the first one was miss-classified and, therefore, the accuracy is $\frac{3}{4} = $ \textbf{0.75} and the error is \textbf{25\%}.\\

\textbf{2b}: when using $k = 3$ the distance matrix doesn't change but now 3 points "vote" to decide the class to assign to each example. The 3 closest points to each test example are highlighted in green in the following table:
\begin{center}
\includegraphics[scale=0.75]{3-nn-distances.png}
\end{center}
So for the first test example (2.7, 2.7) two of the three closest points are +1 so it will be classified as +1; for the second example (2.5, 1) all three points are +1 so its class is +1; for (1.5, 2.5) all three are -1 so its class -1 and, finally, (1.2, 1) will be classified as +1. \\
Only the last prediction is wrong so the accuracy of the model in this case is also \textbf{0.75} and the error rate is \textbf{25\%}.\\

\textbf{2c}: lets look at the scatter plot shown in the HW description.
\begin{center}
\includegraphics[scale=0.55]{q2-dataplot-ref.png}
\end{center}

What I'll do to find the linear classifier is to take the two points on the bottom-left corner, draw the line that joins them (lets call it $L_1$), take the midpoint ($M_1$) and find the perpendicular line ($L_2$) to $L_1$ that passes through $M_1$. The next image illustrates this:
\begin{center}
\includegraphics[scale=0.75]{q2-dataplot.png}
\end{center}

The two points into consideration are (0.8, 1) and (1, 0.5). The first one belongs to class +1. The slope of $L_1$ is given by 
\[ m_1 = \frac{x2_+ - x2_-}{x1_+ - x1_-} = \frac{1 - 0.5}{0.8 - 1} = -2.5 \]

And the midpoint $M_1$ is given by
\[ (\frac{x1_+ + x1_-}{2}, \frac{x2_+ + x2_-}{2}) = (0.9, 0.75) \]

Finally, to find a perpendicular to a line we need the product of the slopes to be -1. So the slope of $L_2$ is 0.4. So $x2 = 0.4x1 + b$, when evaluating in the midpoint we have that $0.75 = 0.4\times 0.9 + b \rightarrow b = 0.39 $. As a result the line we are looking for is:
\[ f(x) = 0.4x_1 - x_2 + 0.39 \]

The following table shows the value resulting of evaluating $f(x)$ on each data example (both training and test).

\begin{center}
\includegraphics[scale=0.75]{linear.png}
\end{center}

I was expecting to see a perfect accuracy but it wasn't the case. After a while I realized that the problem is that the scale on the axis x1 and x2 in the scatter plot are not equal so, even though $L_2$ seemed like a good fit it really wasn't. The accuracy for the training data set is $\frac{5}{8} = 62.5\%$ while the accuracy in the test data set is \textbf{75\%}.\\

A different way to find $f(x)$ would be to take the same midpoint $M_1$ we already found and take a second midpoint from the segment that joins (1.2, 1.9) and (2, 1.2). The reason for choosing these two points is that it looks like line drawn before passes through its midpoint. Lets call this point $M_2$. Its coordinate would be (1.7, 1.95). \\

Now we just need to find a line that passes through $M_1$ and $M_2$. Following the same steps as before we have that $f(x) = 1.5x1 - x2 - 0.6$.\\

Now lets look at the table evaluating the function on all the examples.
\begin{center}
\includegraphics[scale=0.75]{linear2.png}
\end{center}

This time the accuracy on the training data set is \textbf{100\%} and the accuracy on the test data set is \textbf{75\%}. Since the question is asking about the error:\\
Training error is \textbf{0\%}\\
Testing error is \textbf{25\%}\\

\textbf{2d}: for this particular data set both method perform equally well. In general $k$ Nearest Neighbor is good when you have low dimensional data and many examples but this is not the case here; we do have low dimensional data but also just a few examples. As mentioned earlier in this report, there is no "one size fit all" solution and you usually have to experiment with different methods. Regardless of that, there are some particular scenarios in which one is better than the other. When we have lots of data entries KNN may not be a good fit because the computational complexity could explode. 

\section*{Question 3}

\textbf{3a}: start centroids are (0, 4) and (6, 5). In the first iteration we calculate the distance from the centroids to every point in our data. I'm using Euclidean distance and the results are shown in the following table:

\begin{center}
\includegraphics[scale=0.75]{kmeans-iter0.png}
\captionof{table}{Distances from centroid to data points. First iteration.}
\end{center}

In this table the green cells indicate the minimum distance between the two cluster centroids. As a result the points (1, 3), (1, 2), (2, 1), (2, 2), (2, 3) and (3, 2) are clustered together and the remaining 7 points in another one.\\

The next step is to update the centroid by taking the mean of the points included on each cluster. For the first 6 points the mean is \textbf{(1.83, 2.17)}, and for the other cluster, the new centroid is \textbf{(5, 4.14)}.\\

Now we need to recalculate the distances using the new centroids:
\begin{center}
\includegraphics[scale=0.75]{kmeans-iter1.png}
\captionof{table}{Distances from centroid to data points. Second iteration}
\end{center}

After updating the distance we notice that the elements on each cluster remained the same so if we take the means again we would get the same centroids. Because of this, we can stop at this point. So the final result is that the first 6 points belong to cluster 1 and the other 7 to cluster 2.\\

\textbf{3b}:  the first step is to mark all the objects as unvisited and select a random one. Then we calculate the distance from that selected object to all others and count how many there are in the $\epsilon$-neighborhood. See the following table:

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter0.png}
\captionof{table}{Iteration 0 of the DBSCAN clustering algorithm.}
\end{center}

The items marked on teal color are the ones that belong to the neighborhood. Since there are more than \textit{MinPts} we will mark the selected point as visited and create our first cluster $C1$. Now we iterate over the points in the neighbor (from top to bottom).

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter1.png}
\captionof{table}{Iteration 1 of the DBSCAN clustering algorithm.}
\end{center}

Note that now the distance is calculated from the selected point (1, 2). Since it has 3 objects in its neighborhood we will iterate over those too. 

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter2.png}
\captionof{table}{Iteration 2 of the DBSCAN clustering algorithm.}
\end{center}

The objects in (1, 3) neighborhood were already included so we keep going.

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter3.png}
\captionof{table}{Iteration 3 of the DBSCAN clustering algorithm.}
\end{center}

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter4.png}
\captionof{table}{Iteration 4 of the DBSCAN clustering algorithm.}
\end{center}

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter5.png}
\captionof{table}{Iteration 5 of the DBSCAN clustering algorithm.}
\end{center}

Since (4, 3) hasn't been included so far, we will include it.

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter6.png}
\captionof{table}{Iteration 6 of the DBSCAN clustering algorithm.}
\end{center}

\begin{center}
\includegraphics[scale=0.60]{dbscan-iter7.png}
\captionof{table}{Iteration 7 of the DBSCAN clustering algorithm.}
\end{center}

\begin{center}
\includegraphics[scale=0.57]{dbscan-iter8.png}
\captionof{table}{Iteration 8 of the DBSCAN clustering algorithm.}
\end{center}

At this point we can see that all the unvisited elements will eventually be assigned to cluster $C1$ so we are done.

\begin{center}
\includegraphics[scale=0.57]{dbscan-final.png}
\captionof{table}{Final state of the DBSCAN clustering algorithm.}
\end{center}

This result matches the result obtained by using sklearn with the same hyper-parameters. The code can be found together with this report.\\

\textbf{3c}: consider the initial state

\begin{center}
\includegraphics[scale=0.57]{agnes-0.png}
\captionof{figure}{Initial state for the AGNES clustering algorithm.\\ A cluster is considered the same if it has the same shape and color. So in this plot, all 13 points belong to a different cluster.}
\end{center}

The dissimilarity matrix is as follows:
\begin{center}
\includegraphics[scale=0.47]{agnes-table.png}
\captionof{table}{Dissimilarity matrix for all points.}
\end{center}

The bold numbers are the ones with minimum dissimilarity. After merging we have:
\begin{center}
\includegraphics[scale=0.57]{agnes-1.png}
\captionof{figure}{First merge of clusters.}
\end{center}

The next iteration results in:
\begin{center}
\includegraphics[scale=0.57]{agnes-2.png}
\captionof{figure}{Second merge of clusters.}
\end{center}

And for the next one we merge two clusters twice:
\begin{center}
\includegraphics[scale=0.57]{agnes-3.png}
\captionof{figure}{Third merge of clusters.}
\end{center}

On the fourth iteration we finally have just two clusters that match the ones found in part 3a. 
\begin{center}
\includegraphics[scale=0.57]{agnes-4.png}
\captionof{figure}{Fourth merge of clusters.}
\end{center}

We could perform one more merge and end up with a single cluster. I won't do it because it is obvious what the result is  and the process is already evident.\\

Finally, below there is the dendrogram to illustrate the merging of the different clusters. At the lowest level, cluster $i$ is the $i$-th element in our data.

\begin{center}
\includegraphics[scale=0.57]{dendrogram.png}
\captionof{figure}{Dendrogram for our AGNES algorithm.}
\end{center}


\end{document}
